import os
from openai import OpenAI, AzureOpenAI
import backoff 

completion_tokens = prompt_tokens = 0

api_key = os.getenv("OPENAI_API_KEY", "")
api_base = os.getenv("OPENAI_API_BASE", "")

# åˆå§‹åŒ–ä¸» OpenAI å®¢æˆ·ç«¯
if api_key:
    client = OpenAI(
        api_key=api_key,
        base_url=api_base if api_base else None
    )
    if api_base:
        print("Warning: OPENAI_API_BASE is set to {}".format(api_base))
else:
    print("Warning: OPENAI_API_KEY is not set")
    client = None

# å¤‡ç”¨ API é…ç½®
backup_api_key = os.getenv("BACKUP_OPENAI_API_KEY", "")
backup_api_base = os.getenv("BACKUP_OPENAI_API_BASE", "")

# åˆå§‹åŒ–å¤‡ç”¨ OpenAI å®¢æˆ·ç«¯
if backup_api_key and backup_api_base:
    backup_client = OpenAI(
        api_key=backup_api_key,
        base_url=backup_api_base
    )
else:
    backup_client = None

@backoff.on_exception(backoff.expo, Exception)
def completions_with_backoff(**kwargs):
    try:
        return client.chat.completions.create(**kwargs)
    except Exception as e:
        error_message = str(e)
        error_type = type(e).__name__
        
        # å¯¹æ‰€æœ‰é”™è¯¯éƒ½å°è¯•ä½¿ç”¨å¤‡ç”¨ API
        if backup_client:
            print(f"âš ï¸  é‡åˆ° API é”™è¯¯ ({error_type})ï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨ API")
            print(f"   é”™è¯¯ä¿¡æ¯: {error_message[:200]}...")
            print(f"   å¤‡ç”¨ API Base: {backup_api_base}")
            
            try:
                result = backup_client.chat.completions.create(**kwargs)
                print("âœ“ ä½¿ç”¨å¤‡ç”¨ API æˆåŠŸ")
                return result
            except Exception as backup_error:
                print(f"âœ— å¤‡ç”¨ API ä¹Ÿå¤±è´¥: {str(backup_error)[:200]}")
                raise e  # æŠ›å‡ºåŸå§‹é”™è¯¯
        else:
            print(f"âš ï¸  é‡åˆ° API é”™è¯¯ ({error_type})ï¼Œä½†æœªé…ç½®å¤‡ç”¨ API")
            print(f"   é”™è¯¯ä¿¡æ¯: {error_message[:200]}...")
            print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: BACKUP_OPENAI_API_KEY å’Œ BACKUP_OPENAI_API_BASE")
            raise

def gpt(prompt, model="gpt-4", temperature=0.7, max_tokens=8000, n=1, stop=None) -> list:
    messages = [{"role": "user", "content": prompt}]
    return chatgpt(messages, model=model, temperature=temperature, max_tokens=max_tokens, n=n, stop=stop)
    
def chatgpt(messages, model="gpt-4", temperature=0.7, max_tokens=8000, n=1, stop=None) -> list:
    global completion_tokens, prompt_tokens
    outputs = []
    while n > 0:
        # æŸäº› APIï¼ˆå¦‚é˜¿é‡Œäº‘ï¼‰é™åˆ¶ n çš„æœ€å¤§å€¼ä¸º 4
        cnt = min(n, 4)
        n -= cnt
        res = completions_with_backoff(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens, n=cnt, stop=stop)
        
        # è®°å½•æœ¬æ¬¡è°ƒç”¨çš„ token ä½¿ç”¨æƒ…å†µ
        current_prompt_tokens = 0
        current_completion_tokens = 0
        if hasattr(res, 'usage') and res.usage:
            current_prompt_tokens = res.usage.prompt_tokens
            current_completion_tokens = res.usage.completion_tokens
            total_tokens = res.usage.total_tokens
        else:
            total_tokens = 0
        
        # æ‰“å° token ä½¿ç”¨æƒ…å†µ
        if total_tokens > 0:
            print(f"ğŸ“Š Token ä½¿ç”¨: prompt={current_prompt_tokens}, completion={current_completion_tokens}, total={total_tokens}")
        
        # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
        truncated_count = 0
        for choice in res.choices:
            if choice.finish_reason == 'length':
                truncated_count += 1
        
        if truncated_count > 0:
            print(f"âš ï¸  è­¦å‘Š: {truncated_count}/{cnt} ä¸ªå“åº”å› è¾¾åˆ° max_tokens é™åˆ¶è€Œè¢«æˆªæ–­ (max_tokens={max_tokens})")
        
        # æ–°ç‰ˆ SDK ä½¿ç”¨ç»Ÿä¸€çš„å“åº”æ ¼å¼
        for choice in res.choices:
            try:
                content = choice.message.content
                if content is not None:
                    outputs.append(content)
                else:
                    print(f"âš ï¸  è­¦å‘Š: choice.message.content ä¸º None")
            except Exception as e:
                print(f"âš ï¸  è§£æå“åº”å†…å®¹æ—¶å‡ºé”™: {e}")
                print(f"   choice ç±»å‹: {type(choice)}")
                print(f"   choice å†…å®¹: {choice}")
        
        # log completion tokens
        completion_tokens += current_completion_tokens
        prompt_tokens += current_prompt_tokens
    
    return outputs
    
def gpt_usage(backend="gpt-4"):
    global completion_tokens, prompt_tokens
    if backend == "gpt-4":
        cost = completion_tokens / 1000 * 0.06 + prompt_tokens / 1000 * 0.03
    elif backend == "gpt-3.5-turbo":
        cost = completion_tokens / 1000 * 0.002 + prompt_tokens / 1000 * 0.0015
    elif backend == "gpt-4o":
        cost = completion_tokens / 1000 * 0.00250 + prompt_tokens / 1000 * 0.01
    return {"completion_tokens": completion_tokens, "prompt_tokens": prompt_tokens, "cost": cost}
