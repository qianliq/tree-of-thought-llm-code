#!/usr/bin/env python3
"""
åˆå¹¶å¤šä¸ª JSONL ç»“æœæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
    python merge_results.py --dataset data.jsonl --results result1.jsonl result2.jsonl result3.jsonl --output merged.jsonl
    python merge_results.py --dataset data.jsonl --results result*.jsonl --output merged.jsonl
"""

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Set
from collections import defaultdict


def get_task_id(item: dict) -> str:
    """
    ä»æ•°æ®é¡¹ä¸­æå–ä»»åŠ¡ID
    ä¼˜å…ˆæŸ¥æ‰¾ task_idï¼Œå¦‚æœæ²¡æœ‰åˆ™æŸ¥æ‰¾ question_id
    
    å‚æ•°:
        item: æ•°æ®é¡¹å­—å…¸
    
    è¿”å›:
        ä»»åŠ¡IDå­—ç¬¦ä¸²
    """
    if 'task_id' in item:
        return str(item['task_id'])
    elif 'question_id' in item:
        return str(item['question_id'])
    else:
        raise KeyError(f"æ•°æ®é¡¹ä¸­æ²¡æœ‰æ‰¾åˆ° 'task_id' æˆ– 'question_id' å­—æ®µ: {list(item.keys())}")


def load_jsonl(file_path: str) -> List[dict]:
    """
    åŠ è½½ JSONL æ–‡ä»¶
    
    å‚æ•°:
        file_path: æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        æ•°æ®é¡¹åˆ—è¡¨
    """
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è­¦å‘Š: {file_path} ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
    return data


def save_jsonl(data: List[dict], file_path: str):
    """
    ä¿å­˜ä¸º JSONL æ–‡ä»¶
    
    å‚æ•°:
        data: æ•°æ®é¡¹åˆ—è¡¨
        file_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')


def merge_results(dataset_path: str, result_paths: List[str], output_path: str, verbose: bool = True):
    """
    åˆå¹¶å¤šä¸ªç»“æœæ–‡ä»¶
    
    å‚æ•°:
        dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        result_paths: ç»“æœæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    # åŠ è½½æ•°æ®é›†
    if verbose:
        print(f"ğŸ“– åŠ è½½æ•°æ®é›†: {dataset_path}")
    dataset = load_jsonl(dataset_path)
    
    if not dataset:
        print("âŒ é”™è¯¯: æ•°æ®é›†ä¸ºç©º")
        sys.exit(1)
    
    # è·å–æ•°æ®é›†ä¸­çš„æ‰€æœ‰ä»»åŠ¡IDï¼ˆæŒ‰é¡ºåºï¼‰
    dataset_task_ids = []
    dataset_id_field = None
    
    for item in dataset:
        try:
            task_id = get_task_id(item)
            dataset_task_ids.append(task_id)
            if dataset_id_field is None:
                dataset_id_field = 'task_id' if 'task_id' in item else 'question_id'
        except KeyError as e:
            print(f"âŒ é”™è¯¯: {e}")
            sys.exit(1)
    
    if verbose:
        print(f"âœ“ æ•°æ®é›†åŒ…å« {len(dataset_task_ids)} ä¸ªä»»åŠ¡ (ID å­—æ®µ: {dataset_id_field})")
    
    # åŠ è½½æ‰€æœ‰ç»“æœæ–‡ä»¶
    results_by_task_id = defaultdict(list)
    all_result_task_ids = set()
    
    for result_path in result_paths:
        if verbose:
            print(f"\nğŸ“– åŠ è½½ç»“æœæ–‡ä»¶: {result_path}")
        
        results = load_jsonl(result_path)
        
        if not results:
            if verbose:
                print(f"   âš ï¸  è­¦å‘Š: ç»“æœæ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
            continue
        
        # æ£€æµ‹ç»“æœæ–‡ä»¶ä½¿ç”¨çš„IDå­—æ®µ
        result_id_field = None
        for item in results:
            try:
                task_id = get_task_id(item)
                result_id_field = 'task_id' if 'task_id' in item else 'question_id'
                break
            except KeyError:
                continue
        
        if result_id_field is None:
            print(f"   âš ï¸  è­¦å‘Š: ç»“æœæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä»»åŠ¡IDå­—æ®µï¼Œè·³è¿‡")
            continue
        
        # æ”¶é›†ç»“æœ
        file_task_ids = set()
        for item in results:
            try:
                task_id = get_task_id(item)
                results_by_task_id[task_id].append(item)
                file_task_ids.add(task_id)
                all_result_task_ids.add(task_id)
            except KeyError as e:
                if verbose:
                    print(f"   âš ï¸  è­¦å‘Š: è·³è¿‡æ— æ•ˆæ•°æ®é¡¹: {e}")
        
        if verbose:
            print(f"   âœ“ åŒ…å« {len(file_task_ids)} ä¸ªä»»åŠ¡çš„ç»“æœ")
    
    # æ£€æŸ¥è¦†ç›–ç‡
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ“Š è¦†ç›–ç‡åˆ†æ:")
        print(f"{'='*80}")
    
    dataset_task_id_set = set(dataset_task_ids)
    missing_task_ids = dataset_task_id_set - all_result_task_ids
    extra_task_ids = all_result_task_ids - dataset_task_id_set
    
    coverage = len(all_result_task_ids & dataset_task_id_set) / len(dataset_task_id_set) * 100
    
    if verbose:
        print(f"æ•°æ®é›†ä»»åŠ¡æ€»æ•°: {len(dataset_task_id_set)}")
        print(f"ç»“æœæ–‡ä»¶è¦†ç›–: {len(all_result_task_ids & dataset_task_id_set)}")
        print(f"è¦†ç›–ç‡: {coverage:.2f}%")
    
    if missing_task_ids:
        print(f"\nâš ï¸  ç¼ºå¤±çš„ä»»åŠ¡ ({len(missing_task_ids)} ä¸ª):")
        missing_list = sorted(missing_task_ids, key=lambda x: dataset_task_ids.index(x) if x in dataset_task_ids else float('inf'))
        for i, task_id in enumerate(missing_list[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            idx = dataset_task_ids.index(task_id) if task_id in dataset_task_ids else -1
            print(f"   - {task_id} (ç´¢å¼•: {idx})")
        if len(missing_task_ids) > 10:
            print(f"   ... è¿˜æœ‰ {len(missing_task_ids) - 10} ä¸ª")
    
    if extra_task_ids:
        print(f"\nâš ï¸  é¢å¤–çš„ä»»åŠ¡ (ä¸åœ¨æ•°æ®é›†ä¸­) ({len(extra_task_ids)} ä¸ª):")
        for i, task_id in enumerate(sorted(extra_task_ids)[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   - {task_id}")
        if len(extra_task_ids) > 10:
            print(f"   ... è¿˜æœ‰ {len(extra_task_ids) - 10} ä¸ª")
    
    # æŒ‰æ•°æ®é›†é¡ºåºåˆå¹¶ç»“æœ
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ”„ åˆå¹¶ç»“æœ...")
        print(f"{'='*80}\n")
    
    merged_data = []
    tasks_with_multiple_results = []
    
    for task_id in dataset_task_ids:
        if task_id in results_by_task_id:
            task_results = results_by_task_id[task_id]
            
            if len(task_results) > 1:
                tasks_with_multiple_results.append((task_id, len(task_results)))
                if verbose:
                    print(f"âš ï¸  ä»»åŠ¡ {task_id} æœ‰ {len(task_results)} ä¸ªç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ
            merged_data.append(task_results[0])
        else:
            if verbose:
                print(f"âš ï¸  ä»»åŠ¡ {task_id} æ²¡æœ‰ç»“æœï¼Œè·³è¿‡")
    
    if tasks_with_multiple_results and verbose:
        print(f"\nğŸ“‹ æœ‰ {len(tasks_with_multiple_results)} ä¸ªä»»åŠ¡æœ‰å¤šä¸ªç»“æœ:")
        for task_id, count in tasks_with_multiple_results[:5]:
            print(f"   - {task_id}: {count} ä¸ªç»“æœ")
        if len(tasks_with_multiple_results) > 5:
            print(f"   ... è¿˜æœ‰ {len(tasks_with_multiple_results) - 5} ä¸ª")
    
    # ä¿å­˜åˆå¹¶ç»“æœ
    if verbose:
        print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœåˆ°: {output_path}")
    
    save_jsonl(merged_data, output_path)
    
    if verbose:
        print(f"\n{'='*80}")
        print("âœ“ åˆå¹¶å®Œæˆ!")
        print(f"{'='*80}")
        print(f"è¾“å…¥æ•°æ®é›†: {len(dataset)} ä¸ªä»»åŠ¡")
        print(f"è¾“å‡ºç»“æœ: {len(merged_data)} ä¸ªä»»åŠ¡")
        print(f"è¦†ç›–ç‡: {len(merged_data) / len(dataset) * 100:.2f}%")


def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    print(f"{'='*80}")
    print("ğŸ”„ åˆå¹¶ç»“æœæ–‡ä»¶ - äº¤äº’æ¨¡å¼")
    print(f"{'='*80}\n")
    
    # è¾“å…¥æ•°æ®é›†è·¯å¾„
    while True:
        dataset_path = input("ğŸ“– è¯·è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„: ").strip()
        # ç§»é™¤å¯èƒ½çš„å¼•å·
        dataset_path = dataset_path.strip("'\"")
        
        if not dataset_path:
            print("âŒ æ•°æ®é›†è·¯å¾„ä¸èƒ½ä¸ºç©º")
            continue
        
        if not Path(dataset_path).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
            continue
        
        break
    
    print(f"âœ“ æ•°æ®é›†: {dataset_path}\n")
    
    # é€‰æ‹©è¾“å…¥æ¨¡å¼
    print("ğŸ“ è¯·é€‰æ‹©ç»“æœæ–‡ä»¶è¾“å…¥æ¨¡å¼:")
    print("   1. é€ä¸ªè¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰")
    print("   2. æŒ‡å®šç›®å½•ï¼ˆè‡ªåŠ¨æ”¶é›†ç›®å½•ä¸‹æ‰€æœ‰ .jsonl æ–‡ä»¶ï¼‰")
    
    while True:
        mode = input("\nè¯·é€‰æ‹© (1/2): ").strip()
        if mode in ['1', '2']:
            break
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
    
    result_files = []
    
    if mode == '2':
        # ç›®å½•æ¨¡å¼
        while True:
            result_dir = input("\nğŸ“‚ è¯·è¾“å…¥ç»“æœæ–‡ä»¶ç›®å½•: ").strip().strip("'\"")
            
            if not result_dir:
                print("âŒ ç›®å½•è·¯å¾„ä¸èƒ½ä¸ºç©º")
                continue
            
            dir_path = Path(result_dir)
            if not dir_path.exists():
                print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {result_dir}")
                continue
            
            if not dir_path.is_dir():
                print(f"âŒ ä¸æ˜¯æœ‰æ•ˆçš„ç›®å½•: {result_dir}")
                continue
            
            break
        
        # è¯¢é—®æ˜¯å¦éœ€è¦è¿‡æ»¤
        print("\nğŸ” æ˜¯å¦éœ€è¦è¿‡æ»¤æ–‡ä»¶åï¼Ÿ")
        print("   ç¤ºä¾‹: *_lcb_*.jsonl, *humanevalplus*.jsonl")
        pattern = input("   æ–‡ä»¶æ¨¡å¼ (ç›´æ¥å›è½¦ä½¿ç”¨ *.jsonl): ").strip()
        
        if not pattern:
            pattern = "*.jsonl"
        
        # æ”¶é›†æ–‡ä»¶
        matches = list(dir_path.glob(pattern))
        if matches:
            result_files = [str(f) for f in matches if f.is_file()]
            print(f"\nâœ“ æ‰¾åˆ° {len(result_files)} ä¸ªæ–‡ä»¶:")
            for i, f in enumerate(result_files[:10], 1):
                print(f"   {i}. {Path(f).name}")
            if len(result_files) > 10:
                print(f"   ... è¿˜æœ‰ {len(result_files) - 10} ä¸ªæ–‡ä»¶")
        else:
            print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {pattern}")
            print("æ˜¯å¦ç»§ç»­ä½¿ç”¨é€ä¸ªè¾“å…¥æ¨¡å¼? (y/n): ", end='')
            if input().strip().lower() == 'y':
                mode = '1'
            else:
                sys.exit(1)
    
    if mode == '1':
        # é€ä¸ªè¾“å…¥æ¨¡å¼
        print("\nğŸ“ è¯·è¾“å…¥ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰")
        print("   æç¤º: æ¯è¡Œä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ\n")
        
        line_num = 1
        
        while True:
            prompt = f"   ç»“æœæ–‡ä»¶ #{line_num}: "
            result_path = input(prompt).strip()
            # ç§»é™¤å¯èƒ½çš„å¼•å·
            result_path = result_path.strip("'\"")
            
            if not result_path:
                # ç©ºè¡Œï¼Œç»“æŸè¾“å…¥
                if not result_files:
                    print("âŒ è‡³å°‘éœ€è¦ä¸€ä¸ªç»“æœæ–‡ä»¶")
                    continue
                break
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»å¯¹è·¯å¾„
            path_obj = Path(result_path)
            if path_obj.is_absolute():
                # ç»å¯¹è·¯å¾„ï¼Œç›´æ¥æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if path_obj.exists():
                    if result_path not in result_files:
                        result_files.append(result_path)
                        print(f"      âœ“ æ·»åŠ : {result_path}")
                    else:
                        print(f"      âš ï¸  å·²å­˜åœ¨: {result_path}")
                else:
                    # å¯èƒ½æ˜¯ç»å¯¹è·¯å¾„çš„é€šé…ç¬¦ï¼Œå°è¯•ä½¿ç”¨çˆ¶ç›®å½•è¿›è¡Œ glob
                    if '*' in result_path or '?' in result_path:
                        parent = path_obj.parent
                        pattern = path_obj.name
                        matches = list(parent.glob(pattern))
                        if matches:
                            for match in matches:
                                if str(match) not in result_files:
                                    result_files.append(str(match))
                                    print(f"      âœ“ æ·»åŠ : {match}")
                        else:
                            print(f"      âŒ æ²¡æœ‰åŒ¹é…çš„æ–‡ä»¶: {result_path}")
                    else:
                        print(f"      âŒ æ–‡ä»¶ä¸å­˜åœ¨: {result_path}")
                        print("      æ˜¯å¦ç»§ç»­æ·»åŠ å…¶ä»–æ–‡ä»¶? (y/n): ", end='')
                        if input().strip().lower() != 'y':
                            continue
            else:
                # ç›¸å¯¹è·¯å¾„ï¼Œæ”¯æŒé€šé…ç¬¦
                matches = list(Path('.').glob(result_path))
                if matches:
                    for match in matches:
                        if str(match) not in result_files:
                            result_files.append(str(match))
                            print(f"      âœ“ æ·»åŠ : {match}")
                else:
                    # ä¸æ˜¯é€šé…ç¬¦ï¼Œç›´æ¥æ£€æŸ¥æ–‡ä»¶
                    if Path(result_path).exists():
                        if result_path not in result_files:
                            result_files.append(result_path)
                            print(f"      âœ“ æ·»åŠ : {result_path}")
                        else:
                            print(f"      âš ï¸  å·²å­˜åœ¨: {result_path}")
                    else:
                        print(f"      âŒ æ–‡ä»¶ä¸å­˜åœ¨: {result_path}")
                        print("      æ˜¯å¦ç»§ç»­æ·»åŠ å…¶ä»–æ–‡ä»¶? (y/n): ", end='')
                        if input().strip().lower() != 'y':
                            continue
            
            line_num += 1
    
    if not result_files:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœæ–‡ä»¶")
        sys.exit(1)
    
    # å»é‡å¹¶æ’åº
    result_files = sorted(set(result_files))
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœæ–‡ä»¶å + _merged åç¼€
    first_result = Path(result_files[0])
    output_path = str(first_result.parent / f"{first_result.stem}_merged{first_result.suffix}")
    
    print(f"\n{'='*80}")
    print("ğŸ“ æ–‡ä»¶ä¿¡æ¯:")
    print(f"{'='*80}")
    print(f"æ•°æ®é›†: {dataset_path}")
    print(f"ç»“æœæ–‡ä»¶ ({len(result_files)} ä¸ª):")
    for f in result_files:
        print(f"   - {f}")
    print(f"è¾“å‡º: {output_path}")
    
    # ç¡®è®¤
    print(f"\næ˜¯å¦ç»§ç»­åˆå¹¶? (y/n): ", end='')
    if input().strip().lower() != 'y':
        print("å·²å–æ¶ˆ")
        sys.exit(0)
    
    print()
    
    # æ‰§è¡Œåˆå¹¶
    merge_results(dataset_path, result_files, output_path, verbose=True)


def main():
    parser = argparse.ArgumentParser(
        description='åˆå¹¶å¤šä¸ª JSONL ç»“æœæ–‡ä»¶ï¼ŒæŒ‰æ•°æ®é›†é¡ºåºè¾“å‡º',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # äº¤äº’æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    python merge_results.py
    
    # å‘½ä»¤è¡Œæ¨¡å¼
    python merge_results.py --dataset data/humanevalplus.jsonl \\
        --results result1.jsonl result2.jsonl result3.jsonl \\
        --output merged.jsonl
    
    # ä½¿ç”¨é€šé…ç¬¦
    python merge_results.py --dataset data/lcb.jsonl \\
        --results logs/code/*_lcb_*.jsonl \\
        --output merged_lcb.jsonl
    
    # ç›®å½•æ¨¡å¼ï¼ˆè‡ªåŠ¨æ”¶é›†ç›®å½•ä¸‹æ‰€æœ‰ .jsonl æ–‡ä»¶ï¼‰
    python merge_results.py --dataset data/humanevalplus.jsonl \\
        --result-dir logs/code \\
        --output merged.jsonl
    
    # ç›®å½•æ¨¡å¼ + æ¨¡å¼è¿‡æ»¤
    python merge_results.py --dataset data/lcb.jsonl \\
        --result-dir logs/code \\
        --pattern "*_lcb_*.jsonl" \\
        --output merged_lcb.jsonl
    
    # é™é»˜æ¨¡å¼
    python merge_results.py --dataset data.jsonl \\
        --results result*.jsonl \\
        --output merged.jsonl \\
        --quiet
        """
    )
    
    parser.add_argument('--dataset', '-d', 
                        help='æ•°æ®é›† JSONL æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--results', '-r', nargs='+',
                        help='ç»“æœ JSONL æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥å¤šä¸ªï¼‰')
    parser.add_argument('--result-dir', '--dir',
                        help='ç»“æœæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆè‡ªåŠ¨æ”¶é›†è¯¥ç›®å½•ä¸‹æ‰€æœ‰ .jsonl æ–‡ä»¶ï¼‰')
    parser.add_argument('--pattern', '-p',
                        help='æ–‡ä»¶åæ¨¡å¼ï¼ˆç”¨äºè¿‡æ»¤ --result-dir ä¸­çš„æ–‡ä»¶ï¼Œå¦‚ "*_lcb_*.jsonl"ï¼‰')
    parser.add_argument('--output', '-o',
                        help='è¾“å‡ºåˆå¹¶åçš„ JSONL æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quiet', '-q', action='store_true',
                        help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼
    if not args.dataset or (not args.results and not args.result_dir):
        interactive_mode()
        return
    
    # å‘½ä»¤è¡Œæ¨¡å¼
    if not args.output:
        print("âŒ é”™è¯¯: å‘½ä»¤è¡Œæ¨¡å¼éœ€è¦æŒ‡å®š --output å‚æ•°")
        sys.exit(1)
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.dataset).exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset}")
        sys.exit(1)
    
    # æ”¶é›†ç»“æœæ–‡ä»¶
    result_files = []
    
    # å¦‚æœæŒ‡å®šäº†ç›®å½•æ¨¡å¼
    if args.result_dir:
        result_dir = Path(args.result_dir)
        if not result_dir.exists():
            print(f"âŒ é”™è¯¯: ç»“æœç›®å½•ä¸å­˜åœ¨: {args.result_dir}")
            sys.exit(1)
        
        if not result_dir.is_dir():
            print(f"âŒ é”™è¯¯: ä¸æ˜¯æœ‰æ•ˆçš„ç›®å½•: {args.result_dir}")
            sys.exit(1)
        
        # ä½¿ç”¨æ¨¡å¼è¿‡æ»¤ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰ .jsonl æ–‡ä»¶
        pattern = args.pattern if args.pattern else "*.jsonl"
        
        if not args.quiet:
            print(f"ğŸ“‚ ä»ç›®å½•æ”¶é›†ç»“æœæ–‡ä»¶: {args.result_dir}")
            print(f"   æ–‡ä»¶æ¨¡å¼: {pattern}")
        
        # æ”¶é›†ç›®å½•ä¸‹çš„æ‰€æœ‰åŒ¹é…æ–‡ä»¶
        matches = list(result_dir.glob(pattern))
        if matches:
            result_files.extend([str(f) for f in matches if f.is_file()])
            if not args.quiet:
                print(f"   æ‰¾åˆ° {len(result_files)} ä¸ªæ–‡ä»¶")
        else:
            print(f"âš ï¸  è­¦å‘Š: ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {pattern}")
    
    # å¦‚æœæŒ‡å®šäº†å…·ä½“çš„ç»“æœæ–‡ä»¶
    if args.results:
        for pattern in args.results:
            matches = list(Path('.').glob(pattern))
            if matches:
                result_files.extend([str(f) for f in matches])
            else:
                # å¦‚æœä¸æ˜¯é€šé…ç¬¦ï¼Œç›´æ¥æ·»åŠ 
                if Path(pattern).exists():
                    result_files.append(pattern)
                else:
                    print(f"âš ï¸  è­¦å‘Š: ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {pattern}")
    
    if not result_files:
        print(f"âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
        sys.exit(1)
    
    # å»é‡
    result_files = sorted(set(result_files))
    
    if not args.quiet:
        print(f"{'='*80}")
        print("ğŸ“ æ–‡ä»¶ä¿¡æ¯:")
        print(f"{'='*80}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"ç»“æœæ–‡ä»¶ ({len(result_files)} ä¸ª):")
        for f in result_files:
            print(f"   - {f}")
        print(f"è¾“å‡º: {args.output}")
        print()
    
    # åˆå¹¶
    merge_results(args.dataset, result_files, args.output, verbose=not args.quiet)


if __name__ == '__main__':
    main()
